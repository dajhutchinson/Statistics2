---
title: "Compupter Practical 3"
subtitle: "Statistics 2"
author: "Dom Hutchinson"
header-includes:
   - \usepackage{dsfont}
   - \usepackage{fancyhdr}
   - \pagestyle{fancy}
   - \renewcommand{\headrulewidth}{0pt}
   - \fancyhead[L]{Dom Hutchinson}
   - \fancyhead[C]{Statistics 2 - Computer Practical 3}
   - \fancyhead[R]{\today}
   - \newcommand{\dotprod}[0]{\boldsymbol{\cdot}}
   - \newcommand{\cosech}[0]{\mathrm{cosech}\ }
   - \newcommand{\cosec}[0]{\mathrm{cosec}\ }
   - \newcommand{\sech}[0]{\mathrm{sech}\ }
   - \newcommand{\prob}[0]{\mathbb{P}}
   - \newcommand{\nats}[0]{\mathbb{N}}
   - \newcommand{\cov}[0]{\mathrm{Cov}}
   - \newcommand{\var}[0]{\mathrm{Var}}
   - \newcommand{\expect}[0]{\mathbb{E}}
   - \newcommand{\reals}[0]{\mathbb{R}}
   - \newcommand{\integers}[0]{\mathbb{Z}}
   - \newcommand{\indicator}[0]{\mathds{1}}
   - \newcommand{\nb}[0]{\textit{N.B.} }
   - \newcommand{\ie}[0]{\textit{i.e.} }
   - \newcommand{\eg}[0]{\textit{e.g.} }
   - \newcommand{\X}[0]{\textbf{X}}
   - \newcommand{\x}[0]{\textbf{x}}
   - \newcommand{\iid}[0]{\overset{\text{iid}}{\sim}}
   - \newcommand{\proved}[0]{$\hfill\square$\\}
output:
  pdf_document: 
     fig_width: 6
     fig_height: 4
  html_notebook: default
  html_document: default
  word_document: default
---
```{r}
options(warn=-1)
diabetes<-read.csv("data/diabetes_data.csv",header=T) # load data set

missing<-function(v){ # Replace missing values with median of non-zero values
   med<-median(v[v>0])
   v[v==0]<-med
   return(v)
}

diabetes$Glucose<-missing(diabetes$Glucose)
diabetes$BloodPressure<-missing(diabetes$BloodPressure)
diabetes$Insulin<-missing(diabetes$Insulin)
diabetes$SkinThickness<-missing(diabetes$SkinThickness)
diabetes$BMI<-missing(diabetes$BMI)
```

# Question 1
Here I shall test the hypotheses
\begin{center}$H_0:$ BloodPressure data can be modelled by a $\mathcal{N}(\mu,\sigma^2)$ distribution\\ $H_1:$ BloodPressure data can \textbf{not} be modelled by a $\mathcal{N}(\mu,\sigma^2)$ distribution\end{center}
I shall use the maximum likelihood estimates for $\mu$ and $\sigma^2$.
$$\hat{\mu}_\text{MLE}=\frac{1}{n}\sum_{i=1}^nx_i\text{ and }\hat\sigma^2=\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2$$
Define the test statistic
$$T_\text{pearson}(\X)=\sum_{j=1}^m\dfrac{(o_j-e_j)^2}{e_j}\to_\mathcal{D}\chi^2_r$$
where $o_j$ is the number of observations of class $j$ and $e_j$ is the expected number of observations of class $j$, assuming $H_0$ is true.\
In this case $r=m-3=7-3=4$ since we have seven classes of observations but three constraints on these classes due to the assumption of a normal distribution (mean, variance \& sum of class sizes).
```{r}
breaks<-c(-Inf,seq(45,95,by=10),Inf) # Quantise data
obs<-table(cut(diabetes$BloodPressure,breaks))

# Perform Pearson's Goodness of Fit Test
n<-length(diabetes$BloodPressure) # number of data points
r<-length(obs)-3 # degrees of freedom
mu<-sum(diabetes$BloodPressure)/n # MLE mean
sigma<-sqrt(sum((diabetes$BloodPressure-mu)^2)/n) # MLE variance

exp<-n*(pnorm(breaks[-1],mean=mu,sd=sigma)-pnorm(breaks[-length(breaks)],mean=mu,sd=sigma)) # expected values
round(cbind(obs,exp),1) # return data table

t_obs<-sum((obs-exp)^2/exp) # observered test statistic
p_val<-1-pchisq(t_obs,df=r) # p-value
cat("mu=",mu,"\nsigma=",sigma,"\ndf=",r,"\nt_obs=",t_obs,"\np_val=",p_val,sep="") # return test values
```
Here the $p$-value ($`r p_val`$) is not statistically significant enough to reject $H_0$, for a reasonable significance level.\
Thus we accept that \textit{BloodPressure} can be modelled by a Normal distribution.

# Question 2
Let $X_1,\dots,X_n\iid\text{Normal}(\mu,12^2)$ model the \textit{blood pressure} of members of the study.\
Here I shall test the hypotheses
$$H_0:\mu=70\text{ against }H_1:\mu>70$$
Define test statistic
$$T(\X)=\dfrac{\bar{\X}-\mu_0}{\sigma/\sqrt{n}}=\dfrac{\bar{\X}-70}{12/\sqrt{768}}\to_\mathcal{D}\text{Normal}(0,1)$$
This convergence in distribution is a result of the central limit theorem.

```{r}
mu<-70; sigma<-12
n<-length(diabetes$BloodPressure) # number of data points
x_bar<-mean(diabetes$BloodPressure)

t_obs<-(x_bar-mu)/(sigma/sqrt(n)) # observered test statistic
p_val<-1-pnorm(t_obs) # p-value
cat("mu=",mu,"\nsigma=",sigma,"\nx_bar=",x_bar,"\nt_obs=",t_obs,"\np_val=",p_val,sep="") # display test values
```
Here the $p$-value ($`r p_val`$) is statistically significant enough to reject $H_0$, for a reasonable significance level.\
Thus we accept the alternative hypothesis, that $\mu>70$.

# Question 3
Let $Y_i\overset{\text{ind}}{\sim}\text{Bernoulli}(\sigma(\theta^Tx_i))$ for $i\in[1,n]$ $\pi_i=\prob(Y_i=1)$ where $\sigma(z):=\frac{1}{1+e^{-z}}$.\
Then
\[\begin{array}{rrrl}
&\pi_i&:=&\prob(Y_i=1)\\
&&=&\sigma(\theta^Tx_i)\\
&&:=&\dfrac{1}{1+e^{-\theta^Tx_i}}\\
&&=&\dfrac{1}{1+e^{-\sum_{j=1}^d\theta_jx_{ij}}}\\
\implies&\ln\pi_i&=&\ln\left(\dfrac{1}{1+e^{-\sum_{j=1}^d\theta_jx_{ij}}}\right)\\
&&=&\ln1-\ln(1+e^{-\sum_{j=1}^d\theta_jx_{ij}})\\
&&=&-\ln(1+e^{-\sum_{j=1}^d\theta_jx_{ij}})\\
\text{and}&\ln(1-\pi_i)&=&\ln\left(1-\dfrac{1}{1+e^{-\sum_{j=1}^d\theta_jx_{ij}}}\right)\\
&&=&\ln\left(\dfrac{e^{-\sum_{j=1}^d\theta_jx_{ij}}}{1+e^{-\sum_{j=1}^d\theta_jx_{ij}}}\right)\\
&&=&\ln(e^{-\sum_{j=1}^d\theta_jx_{ij}})-\ln(1+e^{-\sum_{j=1}^d\theta_jx_{ij}})\\
&&=&-\left(\sum_{j=1}^d\theta_jx_{ij}\right)-\ln(1+e^{-\sum_{j=1}^d\theta_jx_{ij}})\\
\implies&\ln\dfrac{\pi_i}{1-\pi_i}&=&\ln(\pi_i)-\ln(1-\pi)\\
&&=&-\ln(1+e^{-\sum_{j=1}^d\theta_jx_{ij}})+\left(\sum_{j=1}^d\theta_jx_{ij}\right)+\ln(1+e^{-\sum_{j=1}^d\theta_jx_{ij}})\\
&&=&\sum_{j=1}^d\theta_jx_{ij}
\end{array}\]

```{r}
# sigmoid function
sigma<-function(z) {
   1/(1+exp(-z))
}

# Log likelihood
ell<-function(theta,X,y) {
   p<-as.vector(sigma(X%*%theta))
   sum(y*log(p)+(1-y)*log(1-p))
}

# score function
score<-function(theta,X,y) {
   p<-as.vector(sigma(X%*%theta))
   as.vector(t(X)%*%(y-p))
}

# MLE
maximise.ell<-function(ell,score,X,y,theta0) {
   optim.out<-optim(theta0, fn=ell, gr=score, X=X, y=y, method="BFGS", control=list(fnscale=-1, maxit=1000, reltol=1e-16))
   return(list(theta=optim.out$par, value=optim.out$value))
}
```

# Question 4
Here I shall test whether the variables \textit{BloodPressure, SkinThichness, Insulin} and \textit{Age} are statistically significant to the development of diabetes.\
To do so I shall test the hypotheses
$$H_0:\pmb\theta:=(\theta_3,\theta_4,\theta_5,\theta_8)=\pmb0\text{ against }H_1:\pmb\theta\neq\pmb0$$
Consider the likelihood ratio statistic
$$\Lambda_n:=\frac{L(\hat{\pmb\theta}_0;\pmb{x})}{L(\hat{\pmb\theta}_\text{MLE};\pmb{x})}$$
where $\hat{\pmb\theta}_0$ is the maximum likelihood estimator under the null hypothesis and $\hat{\pmb\theta}_\text{MLE}$ is the maximum likelihood estimator for the full model.\
Define test statistic
$$T_n(\X):=-2\Lambda_n=-2[\ell(\hat{\pmb\theta}_0;\pmb{X})-\ell(\hat{\pmb\theta}_\text{MLE};\pmb{X})]\sim\chi^2_r$$
where $r=4$ since the null hypothesis specifies restrictions on four variables.
```{r}
X_rest<-cbind(1,as.matrix(diabetes[,c(1,2,6,7)])) # Variables we are not testing (ie assuming others=0)
X_full<-cbind(1,as.matrix(diabetes[,1:8])) # all variables
Y<-diabetes[,9] # outcomes

theta_hat_0.value<-maximise.ell(ell,score,X_rest,Y,rep(0,5))$value # MLE under H0
theta_hat_mle.value<-maximise.ell(ell,score,X_full,Y,rep(0,9))$value # MLE for full model
cat("ell(theta_hat_0): ",theta_hat_0.value,"\nell(theta_hat_mle): ",theta_hat_mle.value,sep="") # output values
```
Using these results we can calculate an observed test statistic
$$T_n(\textbf{x})=-2[\ell(\hat{\pmb\theta}_0;\pmb{x})-\ell(\hat{\pmb\theta}_\text{MLE};\pmb{x})]=-2[(`r round(theta_hat_0.value,2)`)-(`r round(theta_hat_mle.value,2)`)]=`r t_obs<--2*(theta_hat_0.value-theta_hat_mle.value); round(t_obs,2)`$$
Since $T_n(\X)\sim\chi^2_4$ we have an observered $p$-value of
$$p(\x):=\prob(T_n(\X)\geq T_n(\x);H_0)=\prob(\chi^2_4\geq`r round(t_obs,2)`)=`r p_val<-1-pchisq(t_obs,4);round(p_val,4)`$$
Using the code described in the epilogue we can confirm this calculation.
```{r}
model1<-glm(Y~X_full,family=binomial) #full model
model2<-glm(Y~X_rest,family=binomial) #restricted model
suppressMessages(library(lmtest)) # load library
lrtest(model1, model2) # perform linear regression test
```
Here the $p$-value ($`r round(p_val,4)`$) is not statistically significant enough to reject $H_0$, for a reasonable significance level.\
Thus we accept that the variables \textit{BloodPressure, SkinThichness, Insulin} and \textit{Age} are \underline{not} statistically significant for the development of diabetes and thus $(\theta_3,\theta_4,\theta_5,\theta_8)=\pmb0$.

# Question 5
```{r}
set.seed(779543035) # Set RNG seed

generate.ys<-function(X,theta) { # Generate new outcomes from data
   n<-dim(X)[1]
   rbinom(n,size=1,prob=sigma(X%*%theta))
}

simulate<-function(theta_hat_mle) {
   new_Y<-generate.ys(X_rest,theta_hat_mle) # Generate new outcomes
   
   theta_hat_0.value<-maximise.ell(ell,score,X_rest,new_Y,rep(0,5))$value # MLE under H0
   theta_hat_mle.value<-maximise.ell(ell,score,X_full,new_Y,rep(0,9))$value # MLE under full model
   
   t_obs<--2*(theta_hat_0.value-theta_hat_mle.value) # observed test statistic
}

n_trials<-500
theta_hat_mle<-maximise.ell(ell,score,X_rest,Y,rep(0,5))$theta
simulation.raw<-sapply(1:n_trials, function(i) simulate(theta_hat_mle)) # Run simulation
```

## a)
```{r}
x<-seq(0,20,0.1)
plot(x,dchisq(x,4),type="l",col="darkgreen",xlab="T(x)",ylab=""
     ,main="Comparision of density of observed statistics & chi^2_4 distribution") # Plot chi^2_m distribution
lines(density(simulation.raw),col="red",lty=2) # Plot distribution of obsered test statistics
legend("topright",legend=c("chi^2_4","Simulation"),lty=1:2,col=c("darkgreen","red"))
```

## b)
Here I shall test the hypotheses
$$H_0:-2\ln\Lambda_n\sim\chi^2_4\text{ against }H_1:-2\ln\Lambda_n\not\sim\chi^2_4$$
This shall be done using Pearson's Goodness-of-Fit test. Usint test statistic
$$T_\text{pearson}(\X)=\sum_{j=1}^m\dfrac{(o_j-e_j)^2}{e_j}\to_\mathcal{D}\chi^2_r$$
In this case $r=12-1=11$ since I split the observed data into twelve classes and there is a single constraint on these classes (sum of class sizes).
```{r}
breaks<-c(-Inf,seq(1,11,by=1),Inf) # quantise data
obs<-table(cut(simulation.raw,breaks))

exp<-n_trials*(pchisq(breaks[-1],4)-pchisq(breaks[-length(breaks)],4)) # Expected number of observations
round(cbind(obs,exp),1) # Return data table

r<-length(obs)-1
t_obs<-sum((obs-exp)^2/exp) # observered test statistic
p_val<-1-pchisq(t_obs,df=r) # p-value
cat("df=",r,"\nt_obs=",t_obs,"\np_val=",p_val,sep="") # return test values
```
Here the $p$-value ($`r p_val`$) is not statistically significant enough to reject $H_0$, for a reasonable significance level.\
Thus we accept that the test statistic $-2\ln\Lambda_n$ is distributed according to $\chi^2_4$.