---
title: "Problem Sheet 7"
subtitle: "Statistics 2"
author: "Dom Hutchinson"
header-includes:
   - \usepackage{dsfont}
output:
  pdf_document: 
     fig_width: 6
     fig_height: 4
  html_notebook: default
  html_document: default
  word_document: default
---
# Question 1

## a)
By assuming that the colour each bulb comes up is independent we can define  $X\sim\text{Binomial}(12,p)$ to represent the number of blubs which come up red.\
Define simple hypotheses $H_0:p=0.25$ (\textit{i.e.} $ii)$ is true) and $H_1:p=0.6$ (\textit{i.e.} $i)$ is true)\
Define test statisic $T(X)=X$.\
Since the farmer decides to accept $i)$ if 8 or more bulbs come up red, we have critical value $c=8$ and critical region $R=[8,12]$.

## b)
Power Function
$$\pi(\theta;T,c)=\pi(p;X,8)=\mathbb{P}(X\geq8;p)=\sum_{i=8}^{12}{12\choose i}p^i(1-p)^{12-i}$$
Significance Level
$$\alpha=\mathbb{P}(\text{Type I Error})=\pi(0.25)=`r 1-pbinom(7,12,.25)`$$
Type II Error Level
$$\beta=\mathbb{P}(\text{Type II Error})=1-\pi(0.6)=`r pbinom(8,12,.6)`$$

## c)
```{r}
x<-seq(0,1,length.out=100)
y<-sapply(x,function(p) 1-pbinom(8,12,p))
plot(x,y,type="l",xlab="True value of p",ylab=expression(pi(p)),main="Power Function for n=12 & c=8")
abline(v=.25)
text(0.25,1,labels="H0: p=0.25",pos=4)
abline(v=.6)
text(0.6,0,labels="H1: p=0.6",pos=4)
```

## d)
Define $T_{NP}(x)$ to be the Neyman-Pearson test. Then
\[\begin{array}{rcl}
T_{NP}(x)&=&\dfrac{L(p=0.6;x)}{L(p=0.25;x)}\\
&=&\dfrac{f_X(x;p=0.6)}{f_X(x;p=0.25)}\\
&=&\dfrac{{12\choose x}(0.6)^x(0.4)^{12-x}}{{12\choose x}(0.25)^x(0.75)^{12-x}}\\
&=&\left(\dfrac{12}{5}\right)^x\left(\dfrac{8}{15}\right)^{12-x}\\
&=&\dfrac{12^x8^{12-x}}{5^x15^{12-x}}\\
&=&\dfrac{2^{2x}3^x2^{3(12-x)}}{5^x3^{12-x}5^{12-x}}\\
&=&\dfrac{2^{36-x}3^{2x-12}}{5^{12}}
\end{array}\]
Thus $T_{NP}(x)$ is an increasing function with $x$.\
In \textbf{a)} we defined $T(X)=X$ meaning $T(X)$ is an equivalent test statistic to $T_{NP}(X)$. Meaning the farmer's test statistic is optimal in the Neyman-Pearson sense.\

## e)
Consider the power function of $(T,c)$ with $n$ \& $c$ not fixed. Then
$$\pi_n(p;X,c):=\sum_{i=c}^n{n\choose i}p^i(1-p)^{n-i}$$
We want a test $(T,c)$ with signifiance level $\alpha=0.05$ and rate of type II error $\beta=0.1$.\
Then we want to find $n$ \& $c$ such that both the following equalities are satisfied
$$\pi_n(0.25;X,c)=0.05\quad\text{and}\quad1-\pi_n(0.6;X,c)=0.1$$
Noting that
$$\pi_n(0.25;X,c)=\sum_{i=c}^n{n\choose i}\left(\frac{1}{4}\right)^i\left(\frac{3}{4}\right)^{n-i}\quad\text{and}\quad\pi_n(0.6;X,c)=\sum_{i=c}^n{n\choose i}\left(\frac{3}{5}\right)^i\left(\frac{2}{5}\right)^{n-i}$$
Using trial-and-error
\begin{tabular}{|c|c|c|c|}
\hline
$n$&$c$&$\pi_n(0.25)$&$1-\pi_n(0.6)$\\
\hline
15&4&0.002&0.539\\
15&5&0.009&0.314\\
15&6&0.034&0.148\\
15&7&0.057&0.095\\
\hline
\end{tabular}
For $n=15$ \& $c=7$ we have significance level $\alpha=0.057$ \& rate of type-II-error $\beta=0.095$ which are both within 1 percentage point of out targets of $0.05$ \& $0.1$ respectively.
```{r}
x<-seq(0,1,length.out=100)
y<-sapply(x,function(p) 1-pbinom(7,15,p))
plot(x,y,type="l",xlab="True value of p",ylab=expression(pi(p)),main="Power Function for n=15 & c=7")
abline(v=.25)
text(0.25,1,labels="H0: p=0.25",pos=4)
abline(v=.6)
text(0.6,0,labels="H1: p=0.6",pos=4)
```

# Question 2

Consider a test between two simple hypotheses. For each of the following statistical models, derive the Neyman-Pearson optimal test statistic, and try to find the simplest equivalent representation.

## a)
Let $\textbf{X}\overset{\text{iid}}{\sim}\text{Poisson}(\lambda)$ and consider the tests $H_0:\lambda=\lambda_0$ \& $H_1:\lambda=\lambda_1$ with $0<\lambda_1<\lambda_0$.Then
\[\begin{array}{rcl}
T_{NP}(\textbf{x})&=&{\displaystyle\prod_{i=1}^n\frac{f_X(x_i;\lambda_1)}{f_X(x_i;\lambda_0)}}\\
&=&{\displaystyle\prod_{i=1}^n\frac{\frac{e^{-\lambda}\lambda_1^{x_i}}{x_i!}}{\frac{e^{-\lambda}\lambda_2^{x_i}}{x_i!}}}\\
&=&{\displaystyle\prod_{i=1}^ne^{\lambda_0-\lambda_1}\left(\frac{\lambda_1}{\lambda_0}\right)^{x_i}}\\
&=&e^{n(\lambda_0-\lambda_1)}\left(\frac{\lambda_1}{\lambda_0}\right)^{\sum x_i}
\end{array}\]
Since $\lambda_1<\lambda_0$ then $\frac{\lambda_1}{\lambda_0}<0$ then $T_{NP}(\textbf{x})$ is increasing with $-S_n(\textbf{x}):=-\sum x_i$.\\
Meaning $T(\textbf{x}):=-S_n(\textbf{x})$ is an equivalent statistic to $T_{NP}$.\

## b)
Let $\textbf{X}\overset{\text{iid}}{\sim}\text{Exponential}(\lambda)$ and consider the tests $H_0:\lambda=\lambda_0$ \& $H_1:\lambda=\lambda_1$ with $0<\lambda_0<\lambda_1$.Then
\[\begin{array}{rcl}
T_{NP}(\textbf{x})&=&{\displaystyle\prod_{i=1}^n\frac{\mathds{1}\{x\geq0\}\lambda_1e^{-\lambda_1x_i}}{\mathds{1}\{x\geq0\}\lambda_0e^{-\lambda_0x_i}}}\\
&=&\displaystyle{\frac{\mathds{1}\{x\geq0\}}{\mathds{1}\{x\geq0\}}\prod_{i=1}^n\frac{\lambda_1e^{-\lambda_1x_i}}{\lambda_0e^{-\lambda_0x_i}}}\\
&=&\displaystyle{\frac{\mathds{1}\{x\geq0\}}{\mathds{1}\{x\geq0\}}\left(\frac{\lambda_1}{\lambda_0}\right)}\prod_{i=1}^ne^{x_i(\lambda_0-\lambda_1)}\\
&=&\displaystyle{\frac{\mathds{1}\{x\geq0\}}{\mathds{1}\{x\geq0\}}\left(\frac{\lambda_1}{\lambda_0}\right)}e^{(\lambda_0-\lambda_1)\sum x_i}
\end{array}\]
Note that $T_{NP}$ is undefined if $\exists\ x_i\in\textbf{x}\text{ st }x_i<0$.\
Otherwise, since $\lambda_0<\lambda_1\implies\lambda_0-\lambda_1<0$ meaning $T_{NP}$ is increasing with $-S_n(\textbf{x}):=\sum x_i$.\
Thus, $T(\textbf{x}):=-S_n(\textbf{x})$ is an equivalent statistic to $T_{NP}(\textbf{x})$.\

## c)
Let $\textbf{X}\overset{\text{iid}}{\sim}\text{Normal}(\mu,\sigma^2)$ with $\sigma^2$ known and consider the tests $H_0:\mu=\mu_0$ \& $H_1:\mu=\mu_1$ with $\mu_0<\mu_1$. Then
\[\begin{array}{rcl}
T_{NP}(\textbf{x})&=&{\displaystyle\prod_{i=1}^n\frac{e^{-\frac{(x_i-\mu_1)^2}{2\sigma^2}}}{e^{-\frac{(x_i-\mu_0)^2}{2\sigma^2}}}}\\
&=&{\displaystyle\prod_{i=1}^ne^{-\frac{1}{2\sigma^2}\left[(x_i-\mu_0)^2-(x_i-\mu_1)^2\right]}}\\
&=&{\displaystyle e^{-\frac{1}{2\sigma^2}\sum[(x_i-\mu_0)^2-(x_i-\mu_1)^2]}}\\
&=&{\displaystyle e^{-\frac{1}{2\sigma^2}\sum[2x_i(\mu_1-\mu_0)+\mu_0^2-\mu_1^2]}}\\
&=&{\displaystyle e^{-\frac{1}{2\sigma^2}\left[n\mu_0^2-n\mu_1^2+2(\mu_1-\mu_0)\sum x_i\right]}}
\end{array}\]
By the constraints we know that $\mu_1-\mu_0>0$ meaning $T_{NP}$ is increasing with $-S_n(\textbf{x}):=\sum x_i$.\
Thus, $T(\textbf{x}):=-S_n(\textbf{x})$ is an equivalent statistic to $T_{NP}(\textbf{x})$.\

## d)
Let $\textbf{X}\overset{\text{iid}}{\sim}\text{Normal}(\mu,\sigma^2)$ with $\mu$ known and consider the tests $H_0:\sigma^2=\sigma^2_0$ \& $H_1:\sigma^2=\sigma^2_1$ with $0<\sigma^2_0<\sigma^2_1$. Then
\[\begin{array}{rcl}
T_{NP}(\textbf{x})&=&{\displaystyle\prod_{i=1}^n\frac{e^{-\frac{(x_i-\mu)^2}{2\sigma_1^2}}}{e^{-\frac{(x_i-\mu)^2}{2\sigma_0^2}}}}\\
&=&{\displaystyle\prod_{i=1}^ne^{-(x_i-\mu)^2\left[\frac{1}{2\sigma^2_0}-\frac{1}{2\sigma^2_1}\right]}}\\
&=&{\displaystyle e^{-\left[\frac{1}{2\sigma^2_0}-\frac{1}{2\sigma^2_1}\right]\sum(x_i-\mu)^2}}
\end{array}\]
Since $\sigma_0^2<\sigma_1^2\implies0<\frac{1}{2\sigma_0^2}-\frac{1}{2\sigma_1^2}$
Thus $T_{NP}$ is increasing with $-\sum(x_i-\mu)^2$ meaning $T(\textbf{x})=-\sum(x_i-\mu)^2$ is an equivalent statistic to $T_{NP}$.