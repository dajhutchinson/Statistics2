\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{changepage} 

\begin{document}

\pagestyle{fancy}
\setlength\parindent{0pt}
\allowdisplaybreaks

\renewcommand{\headrulewidth}{0pt}

% Cover page title
\title{Statistics 2 - Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

% Header
\fancyhead[L]{Dom Hutchinson}
\fancyhead[C]{Statistics 2 - Notes}
\fancyhead[R]{\today}

% Counters
\newcounter{definition}[section]
\newcounter{example}[section]
\newcounter{notation}[section]
\newcounter{proposition}[section]
\newcounter{proof}[section]
\newcounter{remark}[section]
\newcounter{theorem}[section]

% commands
\newcommand{\dotprod}[0]{\boldsymbol{\cdot}}
\newcommand{\cosech}[0]{\mathrm{cosech}\ }
\newcommand{\cosec}[0]{\mathrm{cosec}\ }
\newcommand{\sech}[0]{\mathrm{sech}\ }
\newcommand{\prob}[0]{\mathbb{P}}
\newcommand{\nats}[0]{\mathbb{N}}
\newcommand{\cov}[0]{\mathrm{cov}}
\newcommand{\var}[0]{\mathrm{Var}}
\newcommand{\expect}[0]{\mathbb{E}}
\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\integers}[0]{\mathbb{Z}}
\newcommand{\indicator}[0]{\mathds{1}}
\newcommand{\nb}[0]{\textit{N.B.} }
\newcommand{\ie}[0]{\textit{i.e.} }
\newcommand{\eg}[0]{\textit{e.g.} }
\newcommand{\X}[0]{\textbf{X}}
\newcommand{\x}[0]{\textbf{x}}
\newcommand{\iid}[0]{\overset{\text{iid}}{\sim}}

\newcommand{\definition}[1]{\stepcounter{definition} \textbf{Definition \arabic{section}.\arabic{definition}\ - }\textit{#1}\\}
\newcommand{\definitionn}[1]{\stepcounter{definition} \textbf{Definition \arabic{section}.\arabic{definition}\ - }\textit{#1}}
\newcommand{\proof}[1]{\stepcounter{proof} \textbf{Proof \arabic{section}.\arabic{proof}\ - }\textit{#1}\\}
\newcommand{\prooff}[1]{\stepcounter{proof} \textbf{Proof \arabic{section}.\arabic{proof}\ - }\textit{#1}}
\newcommand{\example}[1]{\stepcounter{example} \textbf{Example \arabic{section}.\arabic{example}\ - }\textit{#1}\\}
\newcommand{\examplee}[1]{\stepcounter{example} \textbf{Example \arabic{section}.\arabic{example}\ - }\textit{#1}}
\newcommand{\notation}[1]{\stepcounter{notation} \textbf{Notation \arabic{section}.\arabic{notation}\ - }\textit{#1}\\}
\newcommand{\notationn}[1]{\stepcounter{notation} \textbf{Notation \arabic{section}.\arabic{notation}\ - }\textit{#1}}
\newcommand{\proposition}[1]{\stepcounter{proposition} \textbf{Proposition \arabic{section}.\arabic{proposition}\ - }\textit{#1}\\}
\newcommand{\propositionn}[1]{\stepcounter{proposition} \textbf{Proposition \arabic{section}.\arabic{proposition}\ - }\textit{#1}}
\newcommand{\remark}[1]{\stepcounter{remark} \textbf{Remark \arabic{section}.\arabic{remark}\ - }\textit{#1}\\}
\newcommand{\remarkk}[1]{\stepcounter{remark} \textbf{Remark \arabic{section}.\arabic{remark}\ - }\textit{#1}}
\newcommand{\theorem}[1]{\stepcounter{theorem} \textbf{Theorem \arabic{section}.\arabic{theorem}\ - }\textit{#1}\\}
\newcommand{\theoremm}[1]{\stepcounter{theorem} \textbf{Theorem \arabic{section}.\arabic{theorem}\ - }\textit{#1}}

\tableofcontents

% Start of content
\newpage

\section{Estimation}

\subsection{Introduction}

\definition{Probabiltiy Space, $(\Omega,\mathcal{F},\prob)$}
A mathematical construct for modelling the real world. A \textit{Probabilty Space} has three elements
\begin{enumerate}[label=\roman*)]
	\item $\Omega$ - Sample space.
	\item $\mathcal{F}$ - Set of events.
	\item $\prob$ - Probability measure.
\end{enumerate}
and most fulfil the following conditions
\begin{enumerate}[label=\roman*)]
	\item $\Omega\in\mathcal{F}$;
	\item $\forall\ A\in\mathcal{F}\implies A^c\in\mathcal{F}$;
	\item $\forall\ A_0,\dots,A_n\in\mathcal{F}\implies\left(\bigcup\limits_iA_i\right)\in\mathcal{F}$;
	\item $\prob(\Omega)=1$; and,
	\item $\prob\left(\bigcup\limits_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty\prob(A_i)$ for disjoint $A_1,A_2,...$ (Countable Additivity).
\end{enumerate}

\definition{Random Variable}
A function which maps an event in the sample space to a value \eg $X:\Omega\to\reals$.\\

\remark{Probability Density Function for iid Random Variable Vector}
For $\X\sim f_n(\cdot;\theta)$ where each component of $\X$ is independent and identically distribution the probability density function of $\X$ is
$$f_n(\x;\theta)=\prod\limits_{i=1}^nf(x_i;\theta)$$

\definition{Expectation}
The mean value for a random variable. For rv $X$
$$\expect(X):=\sum_{x\in\chi}xf_X(x)\quad\&\quad\expect(X):=\int_\reals xf_X(x)dx$$

\theorem{Expection of a Function}
For a function $g:\reals\to\reals$ and rv $X$ with pmf $f_X$
$$\expect(g(X)):=\sum_{g(x)\in\chi}xf_X(x)\quad\&\quad\expect(g(X)):=\int_\reals g(x)f_X(x)dx$$

\theorem{Expectation of a Linear Operator}
For rv $X$ with pmf $f_X$ \& $a,b\in\reals$
$$\expect(aX+b)=a\expect(X)+b$$

\definition{Variance}
For rv $X$
$$\var(X):=\expect\big[(X-\expect(X))^2\big]=\expect(X^2)-\expect(X)^2$$

\theorem{Variance of a Linear Operator}
For rv $X$ and $a,b\in\reals$
$$\var(aX+b)=a^2\var(X)$$

\definition{Moment of a Random Variable}
For rv $X$ the $n^{th}$ moment of $X$ is defined as $\expect(X^n)$.\\
\nb - $\expect(X^n)\neq\expect(X)^n$.\\

\definition{Covariance}
For rv $X\ \&\ Y$
$$\cov(X,Y):=\expect\big[(X-\expect(X))(Y-\expect(Y))\big]=\expect(XY)-\expect(X)\expect(Y)$$

\theorem{Properties of Covaraince}
Let $X\ \&\ Y$ be independent random varaibles
\begin{enumerate}[label=\roman*)]
	\item $\cov(X,X)=\var(X)$;
	\item $\cov(X,Y)=0$
\end{enumerate}

\theorem{Variance of two Random Variables with linear operators}
$$\var(aX+bY)=a^2\var(X)+b^2\var(Y)+2ab\cov(X,Y)$$

\theorem{Independent Random Variables}
Random variables $X_1,\dots,X_n$ are independent iff
$$\prob(X_1\leq a_1,\dots,X_n\leq a_n)=\prod_{i=1}^n\prob(X_i\leq a_i)\ \forall\ a_1,\dots,a_n\in\reals$$

%TODO frequentist v Bayesian

\section{The Likelihood Function}

\definition{Likelihood Function}
Define $\X\sim f_n(\cdot;\theta^*)$ for some unknown $\theta^*\in\Theta$ and let $\x$ be an observation of $\X$.\\
A \textit{Likelihood Function} is any function, $L(\cdot;\x):\Theta\to[0,\infty)$, which is proportional to the PMF/PDF of the obeserved realisation $\x$.
$$L(\theta;\x):=Cf_b(\x;\theta)\ \forall\ C>0$$
\nb Sometimes this is called the \textit{Observed} Likelihood Function since it is dependent on observed data.\\

\definition{Log-Likelihood Function}
Let $\X\sim f_n(\cdot;\theta^*)$ for some unknown $\theta^*\in\Theta$ and $\x$ be an observation of $\X$.\\
The \textit{Log-Likelihood Function} is the natural log of a \textit{Likelihood Function}
$$\ell(\theta;\x):=\ln f_n(\x;\theta)+C,\ C\in\reals$$

\theorem{Multidiensional Transforms}
Let $\X$ be a continuous random vector in $\reals^n$ with PDF $f_{\X}$; $g:\reals^n\to\reals^n$ be a continuous diferentiable bijection; and, $h:=g^{-1}$.\\
Then $\textbf{Y}=g(\X)$ is a continuous random vector and its PDF is
$$f_{\textbf{Y}}(\textbf{y})=f_{\X}(h(\textbf{y})H_h(\textbf{Y})$$
where
$$J_h:=\left|\det\left(\frac{\partial h}{\partial\textbf{y}}\right)\right|$$

\proposition{Invaraince of Likelihood Function by bijective transformation of the observations independent of $\theta$}
Let $g:\reals^n\to\reals^n$ be a bijetive transformation which is independent of $\theta$; and $\textbf{Y}:=g(\X)$.\\
Then $\textbf{Y}$ is a random variable with PDF/PMG
$$f_{\textbf{Y}}(\textbf{y};\theta)\propto f_{\X}(g^{-1}(\textbf{y});\theta)$$
Hence, if $\textbf{y}=g(\x)$ then $L_{\textbf{Y}}(\theta;\textbf{y})\propto L_{\X}(\theta;\x)$\\

\proof{Proposition 2.1}
Let $g:\reals^n\to\reals^n$ be a bijective transformation which is independent of $\theta$; $h:=g^{-1}$;  $\X,\textbf{Y}$ be a rvs st $\textbf{Y}:=g(\X)$.
\begin{enumerate}[label=\roman*)]
	\item \textit{Discrete Case} - Consider the case when $\X$ is a discrete rv. Then
	\[\begin{array}{rcl}
	f_\textbf{Y}(y;\theta)&=&\prob(\textbf{Y}=\textbf{y};\theta)\\
	&=&\prob(g^{-1}(\textbf{Y})=g^{-1}(\textbf{y});\theta)\\
	&=&\prob(h(\textbf{Y})=h(\textbf{y});\theta)\\
	&=&\prob(\X=h(\textbf{y});\theta)\\
	&=&f_\X(g^{-1}(\textbf{y});\theta)
	\end{array}\]
	\item \textit{Continuous Case} - Consider the case when $\X$ is a continuous rv.\\
	Then, by \textbf{Theorem 2.1}
	$$f_\textbf{Y}(\textbf{y};\theta)=f_\X(g^{-1}(\textbf{y});\theta)J_{g^{-1}}(\textbf{y})$$
	Since $J_{g^{-1}}$ does not depend on $\theta$ this case is solved.
\end{enumerate}
Thus in botoh cases $L_\textbf{Y}(\theta;y)=f_\textbf{Y}(y;\theta)\propto f_\X(g^{-1}(\textbf{y});\theta)=L_\X(\theta;\x)$.

\section{Maximum Likelihood Estimates}

\definition{Maximum Likelihood Estimate}
Let $\X\sim f_n(\cdot;\theta)$; and $\x$ be a realisation of $\X$.\\
The \textit{Maximum Likelihood Estimate} is the value $\hat{\theta}\in\Theta$ st
$$\forall\ \theta\in\Theta\ f_n(\x;\hat{\theta})\geq f_n(\x,\theta)$$
Equivalently
$$\forall\ \theta\in\Theta\ L(\hat{\theta};\x)\geq L(\theta;\x)\quad\mathrm{or}\quad\ell(\hat{\theta};\x)\geq\ell(\theta;\x)$$
\ie $\hat{\theta}(\x):=\mathrm{argmax}_\theta(L(\theta;\x)$.\\

\remark{The Maximum Likelihood Estimate may \underline{not} be unique}

\example{MLE for Uniform Distribution}
Consider $\X{\iid}U[0,\theta]$ for $\theta>0$.\\
Then
\[\begin{array}{rrcl}
&L(\theta;\x)&\propto&f_n(\x;\theta)\\
&&=&\prod\limits_{i=1}^n\dfrac{1}{\theta}\mathds{1}\{x_i\in[0,\theta]\\
&&=&\dfrac{1}{\theta^n}\prod\limits_{i=1}^n\mathds{1}\{x_i\in[0,\theta]\\
\implies&\hat{\theta}&=&\max\{x_i:x_i\in\x\}
\end{array}\]

\remark{MLE of Reparameterisation}
Define $\tau(\theta):\reals\to\reals$. Then
$$\hat{\tau}=\tau(\hat{\theta})$$
\nb We often write $\tilde{f}$ to represent the pmf when $\tau$ is taken as a parameter rateher than $\theta$. \ie $f(x;\theta)=\tilde{f}(x;\tau(\theta))$.\\

\theorem{Invariance of MLE under bijective Reparameterisation}
Let $g:\Theta\to G$ be a bijective transformation of the statisitcal parameter $\theta$.\\
Let $\X\sim f(\cdot;\theta)=\tilde{f}(\cdot;g(\theta))$ for some $\theta$, and let $\x$ be a realisation of $\X$.
\begin{center}
If $\hat{\theta}$ s an MLE of $\theta$ then $\hat{\tau}=g(\hat{\theta})$ is an MLE of $\tau$.
\end{center}

\proof{Theorem 3.1}
\textit{This is a proof by contradiction}.\\
Suppose $\exists\ \tau^*\in G st \tilde{f}(x;\tau^*)>\tilde{f}(x;\tau^*)$
We know that $\forall\ \theta\in\Theta,\ f(x;\theta)=\tilde{f}(x;g(\theta))$ and $\forall\ \tau\in G,\ f(x;g^{-1}(\tau))=\tilde{f}(x;\tau)$.\\
We deduce that
\[\begin{array}{rcl}
f(x;g^{-1}(\tau^*))&=&\tilde{f}(x;\tau^*)\\
&>&\tilde{f}(x;\hat{\tau})\text{ by assumption}\\
&=&f(x;g^{-1}(\hat{\tau}))\\
&=&f(x;\hat{\theta})
\end{array}\]
This contradicts the assumption that $\hat{\theta}$ is an maximum likelihood estimate of $\theta$.\\

\remark{Not all Reparameterisations are Bijective}
When reparameterisations $g:\reals\to\reals$ is not bijective it is helpful to consider the \textit{induced likelihood}
$$L^*(\tau;\x):=\underset{\theta\in G_\tau}{\text{max}}L(\theta;\x)\ \mathrm{where}\ G_\tau:=\{\theta:g(\theta)=\tau\}$$
Since this reduces the domain to only where $g$ is bijective.

\subsection{Determinig MLEs - The Tractable Case}

\proposition{Differentiable Likelihood in the continuous case - Multivariate}
When $L(\theta;\x$ is differentiable one can find MLEs by considering its extrema. This is done equating \& solving the cases when the gradient is zero, \ie $\nabla L(\theta;\x)=0$, and then checking whether this is a maximum or minimum point.\\
A point is a local minimum if the Hessian at the point is \textit{Negative Definite} \ie $x^TAx<0\ \forall\ x\neq\pmb{0}$.\\

\example{MLE of Normal Distribution}
Let $\X{\iid}\mathcal{N}(\mu,\sigma^2)$
\[\begin{array}{rrcl}
&L(\mu,\sigma^2;\x)&=&\prod\limits_{i=1}^n\dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}\\
\implies&\ell(\mu,\sigma^2;\x)&=&C-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\\
\implies&\nabla\ell(\mu,\sigma^2;\x)&=&\left(\dfrac{-1}{\sigma^2}\sum\limits_{i=1}^n(x_i-\mu),\quad-\dfrac{n}{2\sigma^2}+\dfrac{1}{2\sigma^4}\sum\limits_{i=1}^n(x_i-\mu)^2\right)\\
\text{Setting}&\dfrac{-1}{\sigma^2}\sum\limits_{i=1}^n(x_i-\mu)&=&0\\
\implies&\hat{\mu}&=&\dfrac{1}{n}\sum_{i=1}^nx_i=\bar{x}\\
\text{Setting}&-\dfrac{n}{2\sigma^2}+\dfrac{1}{2\sigma^4}\sum\limits_{i=1}^n(x_i-\mu)^2&=&0\\
\implies&\hat{\sigma}^2&=&\dfrac{1}{n}\sum\limits_{i=1}^n(x_i\hat{\mu})^2
\end{array}\]
We now want to check whether $(\hat{\mu},\hat{\sigma^2})$ is a minimum.\\
\[\begin{array}{rcl}
\nabla^2\ell(\mu,\sigma^2;\x)&=&\begin{pmatrix}
\dfrac{\partial^2\ell(\mu,\sigma^2;\x)}{\partial\mu^2}&\dfrac{\partial^2\ell(\mu,\sigma^2;\x)}{\partial\mu\partial\sigma^2}\\
\dfrac{\partial^2\ell(\mu,\sigma^2;\x)}{\partial\mu\sigma^2}&\dfrac{\partial^2\ell(\mu,\sigma^2;\x)}{\partial(\sigma^2)^2}
\end{pmatrix}\\
&=&\begin{pmatrix}
-\dfrac{n}{\hat{\sigma}^2}&0\\
0&-\dfrac{n}{2\hat{\sigma}^4}
\end{pmatrix}
\end{array}\]
Since $\begin{pmatrix}z_1&z_2\end{pmatrix}\begin{pmatrix}-a&0\\0&-b\end{pmatrix}\begin{pmatrix}z_1\\z_2\end{pmatrix}=-az_1^2-bz_2^2<0\ \forall\ a,b>0$ and we have $\frac{n}{\hat{\sigma}^2},\ \frac{n}{2\hat{\sigma}^4}>0$ then we can conclude that $\nabla^2\ell$ is negative definite.\\
Thus $\hat{\mu}=\bar{x}\ \&\ \hat{\sigma}^2=\dfrac{1}{n}\sum\limits_{i=1}^n(x_i\hat{\mu})^2$ is an MLE for the normal distribution.\\

\example{MLE for Capture-Recapture Model}
Suppose you are wanting to calculate the unknown size of a population, $n$. The Capture-Recapture Model is one technique that can be used. You tag $t\leq n$ members of the population; wait for a while; then recapture $c\leq n$ members of which $x\leq\min\{t,c\}\leq n$ are tagged.\\
With $t,c,x$ known produce a MLE for $n$.\\
We first work out the associated probability distribution for $X$, the population size. We have
\begin{enumerate}[label=\roman*)]
	\item ${t \choose x}$ ways of choosing $x$ members among the tagged ones;
	\item ${{n-t} \choose {c-x}}$ ways of choosing the remaining members among the non-tagged ones;
	\item ${n \choose c}$ ways of choosing $c$ members in a population of $n$ individuals.
\end{enumerate}
Thus
$$f_X(x;n)=\frac{{t \choose x}{{n-t} \choose {c-x}}}{{n \choose c}}$$
This means that $X\sim\text{Hypergeometric}(t,n,c)$ with $t\ \&\ c$ known.\\
Now we calculate the MLE for $X$
\[\begin{array}{rcl}
L(n;x)&=&f_X(x;n)\\
&=&\dfrac{{t \choose x}{{n-t} \choose {c-x}}}{{n \choose c}}\\
&=&\dfrac{\dfrac{t!}{x!(t-x)!}\dfrac{(n-t)!}{(c-x)!(n-t-c+x)!}}{\dfrac{n!}{c!(n-c)!}}\\
\end{array}\]
Now we consider $L(n;x)=0$ when $x>\min\{t,c\}$. We want to indetify values of $n$ for which $L(n;x)\geq L(n-1;x)$.\\
Consider $n-1\geq\min\{t,c\}\implies L(n-1;x)>0$
\[\begin{array}{rrrl}
&\text{Let }r(n)&:=&\dfrac{L(n;x)}{L(n-1;x)}\\
&&=&\dfrac{n-t}{n-t-c+x}\dfrac{n-c}{n}\\
\Rightarrow&1&\leq&r(n)\\
\Leftrightarrow&1&\leq&\dfrac{n-t}{n-t-c+x}\dfrac{n-c}{n}\\
\Leftrightarrow&n(n-t-c+x)&\leq&(n-t)(n-c)\\
\Leftrightarrow&n^2-nt-cn+xn&\leq&n^2-nt-cn+ct\\
\Leftrightarrow&xn&\leq&ct\\
\Leftrightarrow&x&\leq&\dfrac{ct}{n}
\end{array}\]
So $L(n;x)$ is increasing for $n\leq\left\lfloor\frac{ct}{x}\right\rfloor$ \& decreasing for $n>\left\lfloor\frac{ct}{x}\right\rfloor$.\\
Consequently $\hat{n}_{\text{MLE}}(x)=\left\lfloor\frac{tc}{x}\right\rfloor$

\section{Statistics and Estimators}

\definition{Statistic}
Given some data $\x$ a statistic is a fucntion of the data $T(\x)$.\\
\nb A statistic cannot depend on an unknown statistical parameter.\\

\definition{Estimate}
Let $\X\sim f_n(\cdot;\theta^*)$ with $\theta^*\in\Theta$ and $\x$ be a realisation of $\X$.\\
An \textit{Estimate} $\theta^*$ is a statistic $\hat{\theta}(\x)=T(\x)$ which is intended to approximate the real value of $\theta^*$.\\
\nb An \textit{Estimate} is a real value \& thus is hard to evaluate.\\

\definition{Estimator}
Let $\X\sim f_n(\cdot;\theta^*)$ with $\theta^*\in\Theta$ and $\x$ be a realisation of $\X$.\\
An \textit{Estimator} of $\theta^*$ is $\hat{\theta}$ where $\hat{\theta}(\x)$ is an \textit{estimate}.\\
\nb We call $T(\X)$ an estimator. This is a random variable.\\

\definition{Distribution of an Estimator}
Let $\X|sim f_n(\cdot;\theta^*)$ with $\theta^*\in\Theta\subseteq\reals$.\\
If $\hat{\theta}(\X)$ is a real-valued random variable, we can write its CDF as
\[\begin{array}{rcl}
F_{\hat{\theta}(\X)}(t;\theta^*)&=&\prob(\hat{\theta}(\X)\leq t;\theta^*)\\
&=&{\displaystyle \int_{\chi^n}\mathds{1}\{\hat{\theta}(\x)\leq t\}f_n(\x;\theta^*)d\x}
\end{array}\]

\remark{Estimator dependends upon true value}
The distribution of $\hat{theta}(\X)$ depends on the distribution of $\X$ which in turn depends upon the distribution of $\theta^*$.\\
Thus the distribution of an estimator depends on the true parameter of the variable it is estimating.\\

\remark{Estimator Distribution \& Sample Size}
As sample size increasesthe distribution of an estimator may converge to a more standard distribution (\eg Normal, Poisson).

\definition{Bias}
\textit{Bias} is a measure of how much an estimator deviates from the true value, on average.
\[\begin{array}{rrl}
\text{Bias}(\hat{\theta};\theta^*)&:=&\expect(\hat{\theta}(\X)-\theta^*;\theta^*)\\
&=&\expect(\hat{\theta};\theta^*)-\expect(\theta^*;\theta^*)\\
&=&\expect(\hat{\theta};\theta^*)-\theta^*
\end{array}\]

\definition{Unbiased Estimator}
An \textit{Estimator}, $\hat{\theta}$, is said to be \textit{Unbiased} if $\forall\ \theta\in\Theta,\ \text{Bias}(\hat{\theta};\theta)=0$.\\
Equivalently $\expect(\hat{\theta};\theta)=\theta$.\\

\definition{Mean Square Error}
The \textit{Mean Square Error} of an estimator is the mean of the squared error associated with rv $\hat{\theta}$.
$$MSE(\hat{\theta};\theta^*):=\expect\left[(\hat{\theta}(\X)-\theta^*)^2;\theta^2\right]$$

\proposition{Simplifcation of MSE Formula}
The MSE is a combination of variance \& bias.
\[\begin{array}{rcl}
MSE(\hat{\theta};\theta^*)&=&\expect\left[(\hat{\theta}(\X)-\theta^*)^2;\theta^2\right]\\
&=&\expect\left[\left\{\hat{\theta}-\expect(\hat{\theta};\theta^*)\right\}^2;\theta^*\right]+\left(\expect(\hat{\theta}-\theta^*;\theta^*\right)^2\\
&=&\text{Var}(\hat{\theta};\theta^*)+\text{Bias}(\hat{\theta};\theta^*)^2
\end{array}\]

\example{Sample mean as an Estimator}
Let $\X{\iid}\text{Poisson}(\lambda^*)$.\\
Suppose we are using the sample mean, $\hat{\lambda}(\x):=\frac{1}{n}\sum_{i=1}^nx_i$, as an estimate of $\lambda^*$. We first want to show this estimator is \textit{Unbiased}
\[\begin{array}{rrl}
\expect(\hat{\lambda};\lambda)&=&\expect\left(\dfrac{1}{n}\sum\limits_{i=1}^nX_i;\lambda\right)\\
&=&d\frac{1}{n}\sum\limits_{i=1}^n\expect(X_i;\lambda)\\
&=&\frac{1}{n}n\lambda\\
&=&\lambda\\
\end{array}\]
Thus $\hat{\lambda}$ is unbiased.\\
Now we consider the MSE of $\hat{\lambda}$
\[\begin{array}{rcl}
MSE(\hat{\lambda};\lambda)&=&\text{Var}(\hat{\lambda};\lambda)\\
&=&\text{Var}\left(\dfrac{1}{n}\sum\limits_{i=1}^nX_i;\lambda\right)\\
&=&\dfrac{1}{n^2}\sum\limits_{i=1}^n\text{Var}(X_i;\lambda)\\
&=&\frac{1}{n^2}n\lambda\\
&=&\frac{\lambda}{n}
\end{array}\]
This shows that as the sample size increases the MSE of $\hat{\lambda}$ converges to $0$.

\section{Probabilistic Convergence}

\remark{Motivation}
Here we consider the properties of a maximum likelihood estimators as the sample size increases.\\

\theorem{Markov's Inequality}
For a \textit{non-negative} random variable $X$ and a constant $a>0$
$$\prob(X\geq a)\leq\dfrac{\expect(X)}{a}$$

\proof{Markov's Inequality}
Consider continuous $X$. We have
\[\begin{array}{rrcl}
&a\prob(X\geq a)&=&a{\displaystyle\int_a^\infty f_X(x)dx}\\
&&\leq&{\displaystyle\int_a^\infty xf_X(x)dx}\\
&&\leq&{\displaystyle\int_0^\infty xf_X(x)dx}\\
&&=&\expect(X)\\
\implies&a\prob(X\geq a)&=&\expect(X)\\
\implies&\prob(X\geq a)&\leq&\dfrac{\expect(X)}{a}
\end{array}\]
$\hfill\square$
% TODO discrete case

\theorem{Chebyshev's Inequality}
Let $\mu=\expect(X)$ and $\sigma^2=\text{Var}(X)$. Then
$$\forall\ a>0,\ \prob(|X-\mu|\geq a)\leq\dfrac{\sigma^2}{a^2}$$

\proof{Chebyshev's Inequality}
We have
\[\begin{array}{rcl}
\prob(|X-\mu|\geq a)&=&\prob(|X-\mu|^2\geq a^2)\\
&\leq&\dfrac{\expect\left((X-\mu)^2\right)}{a^2}\ \text{By Markov's Inequality}\\
&=&\dfrac{\sigma^2}{a^2}
\end{array}\]
$\hfill\square$

\definition{Convergence in Probability}
We say the sequence of random variables $\{Z_n\}_{n\in\nats}$ converges in probability to the random variable $Z$ if
$$\forall\ \varepsilon>0,\ \lim_{n\to\infty}\prob(|Z_n-Z|>\varepsilon)=0$$
\nb This is denoted $Z_n\to_\prob Z$.\\
\nb The random variables $\{Z_n\}_{n\in\nats}$ \& $Z$ must be in the same probability space.\\

\theorem{Weak Law of Large Numbers}
If $\{X_n\}_{n\in\nats}$ are idependent \& identically distributed and $\expect(X_1)=\mu<\infty$ then
$$Z_n=\frac{1}{n}\sum_{i=1}^nX_i\to_\prob\mu$$
\nb This is an example of Convergence in Probability.\\

\definition{Convergence in Distribution}
We say the sequence of random variables $\{Z_n\}_{n\in\nats}$ converges in distribution to random variable $Z$ if
$$\forall\ z\in\text{Z where }\prob(Z\leq z)\text{ is continuous, }\lim_{n\to\infty}\prob(Z_n\leq z)=\prob(Z\leq z)$$
\nb This is denoted $Z_n\to_\mathcal{D} Z$.\\
\nb The random variables $\{Z_n\}_{n\in\nats}$ \& $Z$ need not be in the same probability space.\\

\remark{Equivalent Statements to Convergence in Distribution}
Saying that $Z_n\to_\mathcal{D} Z$ is equivalent to saying that
$$\forall\ z\in\text{Z where }F_Z(z)\text{ is continuous, }\lim_{n\to\infty}F_{Z_n}(z)=F_Z(z)$$

\theorem{Central Limit Theorem}
If $\{X_n\}_{n\in\nats}$ are idependent \& identically distributed, $\expect(X_1)=\mu<\infty$ and $\var(X_1)=\sigma^2<\infty$ then
$$\frac{\sqrt{n}}{\sigma}(Z_n-\mu)\to_\mathcal{D}Z\sim\text{Normal}(0,1)$$

\theorem{Convergence in Probability \& Distribution}
Convergence in probabilitiy $\implies$ Convergence in distribution, \textbf{but} the opposite is not necessarily true.\\

\theorem{Convergence in Probability \& Distribution to a Constant}
Convergence in distribution to a constant \textbf{and} convergence in probability to a constant are equivalent.\\

\example{}
Let $X\sim\text{Bernoulli}\left(\frac{1}{2}\right)$ and $\{X_n\}_{n\in\nats}$ be a sequence of random variables where $X_i:=(1-X)+\frac{1}{n}$.\\
We have
$$F_X(x)=\begin{cases}
0&,\ x<0\\
\frac{1}{2}&,\ x\in[0,1)\\
1&,x\geq1
\end{cases}\quad F_{X_n}(x)=\begin{cases}
0&,\ x<\frac{1}{n}\\
\frac{1}{2}&,\ x\in\left[\frac{1}{n},1+\frac{1}{n}\right)\\
1&,x\geq1+\frac{1}{n}
\end{cases}$$
Clearly $F_{X_n}(x)\to F_X(x)$ at all points at which $F_X$ is continuous (\ie $x\in\reals\backslash\{0,1\}$).\\
Thus $X_n\to_\mathcal{D}X$.\\

\theorem{Continuous Mapping Theorem}
Let $g:Z\to G$ be a \textit{continuous} function. Then
\begin{enumerate}[label=\roman*)]
	\item If $Z_n\to_\prob Z$, then $g(Z_n)\to_\prob g(Z)$;
	\item If $Z_n\to_\mathcal{D} Z$, then $g(Z_n)\to_\mathcal{D} g(Z)$
\end{enumerate}

\theorem{Slutsky's Theorem}
Let $\{Y_n\}_{n\in\nats}\ \&\ \{Z_n\}_{n\in\nats}$ be sequences of random variables, $Y$ be a random variable \& $c\in\reals\backslash0$ be a constant.\\
If $Y_n\to_\mathcal{D}Y$ and $Z_n\to_\mathcal{D}c$, then
\begin{enumerate}[label=\roman*)]
	\item $Y_n+Z_n\to_\mathcal{D}Y+c$;
	\item $Y_nZ_n\to_\mathcal{D}Yc$; and,
	\item $\frac{Y_n}{Z_n}\to_\mathcal{D}\frac{Y}{c}$.
\end{enumerate}

\definition{Convergence in Quadratic Mean}
Let $\{Z_n\}_{n\in\nats}$ be a sequence of random variables \& $Z$ be a random variable.\\
We say that $\{Z_n\}_{n\in\nats}$ \textit{Converges in Quadratic Mean} to the random variable $Z$ if
$$\lim_{n\to\infty}\expect\left[(Z_n-Z)^2\right]=0$$
\nb This is denoted $Z_n\to_{qm}Z$.\\

\theorem{If $Z_n\to_{qm}Z$ then $Z_n\to_\prob Z$}

\proof{Theorem 5.9}
Fix any $\varepsilon>0$. We have
\[\begin{array}{rcl}
\prob(|Z_n-Z|>\varepsilon)&=&\prob(|Z_n-Z|^2>\varepsilon^2)\\
&\leq&\frac{1}{\varepsilon^2}\expect\left[(Z_n-Z)^2\right]\text{ by Markov's Inequality}\\
&\to&0\text{ since $Z_n\to_{qm}Z$.}
\end{array}\]
Hence $Z_n\to_\prob Z$.

\subsection{Probabilistic Convergence \& Estimators}

\definition{Consistency of a Sequence of Estimators}
A sequence of estimators, $\{\hat{\theta}_n(\cdots):\chi^n\to\Theta\}$, are said to be \textit{Consistent} if
$$\forall\ \theta\in\Theta\text{ with }\X_n\sim f_n(\cdot;\theta),\ \hat{\theta}_n(\X_n)\to_{\prob(\cdot;\theta)}\theta$$

\remarkk{Consistency of a Sequence of Estimators}
\begin{enumerate}[label=\roman*)]
	\item In numerous situations one will talk about the consistency of \textit{the} estimator, \eg for the MLE, but also for the mean, etc. This implicitly refers to the corresponding sequence of MLEs, sequence of means, etc.
	\item Note the $\prob(\cdot;\theta)$ in the limit above, and in particular the dependence on $\theta$. This is often omitted in practice, you should however not forget what the symbols actually mean.
	\item Quadratic mean / Mean Square convergence $\implies$ consistency.\\
	That is, if the MSE of the estimator converges to 0, the estimator is consistent.
\end{enumerate}

\example{Consistency of Flipping Coins}
Let $\X{\iid}\text{Bernoulli}(\theta^*)$ for some $\theta^*\in[0,1]$.\\
The maximum likelihood estimate and method of moments for $\hat{\theta}_n$ are the sample mean.\\
$${\hat{\theta}_n(X_1,\dots,X_n)=\frac{1}{n}\sum\limits_{i=1}^nX_i}$$
By the \textit{Weak Law of Large Numbers} we have that \textit{consistency} of $\{\hat{\theta}n\}$ , since $\expect(X_1)=\theta^*$.\\

\example{Crude Confidence Interval when Flipping Coins}
Let $\X{\iid}\text{Bernoulli}(\theta^*)$ for some $\theta^*\in[0,1]$ and define $\hat{\theta}_n:=\hat{\theta}_n(X_1,\dots,X_n)$.\\
We shall produce a \textit{confidence interval} for $\theta^*$.\\
	$$\expect(\hat{\theta}_n;\theta^*)=\theta^*\quad\text{and}\quad\var(\hat{\theta}_n;\theta^*)=\frac{\theta^*(1-\theta^*)}{n}$$
\[\begin{array}{rcll}
\prob\left(|\hat{\theta}_n-\theta^*|\geq\varepsilon;\theta^*\right)&\leq&\frac{\theta^*(1-\theta^*)}{n\varepsilon^2}&\text{by Chebyshev's Inequality}\\
\text{We don't know $\theta^*$, but can deduce that }\theta^*(1-\theta^*)&\leq&\frac{1}{4}\\
\implies\prob\left(|\hat{\theta}_n-\theta^*|\geq\varepsilon;\theta^*\right)&\leq&\frac{1}{4n\varepsilon^2}\\
\text{Define }\alpha&:=&\frac{1}{4n\varepsilon^2}\\
\implies\prob\left(|\hat{\theta}_n-\theta^*|\geq\frac{1}{2\sqrt{n\alpha}};\theta^*\right)&\leq&\alpha\\
\implies\prob\left(\hat{\theta}_n-\frac{1}{2\sqrt{n\alpha}}<\theta^*<\hat{\theta}_n+\frac{1}{2\sqrt{n\alpha}};\theta^*\right)&\geq&1-\alpha
\end{array}\]
This means the random interval $(\hat{\theta}_n-\frac{1}{2\sqrt{n\alpha}},\ \hat{\theta}_n+\frac{1}{2\sqrt{n\alpha}};\theta^*)$ contains $\theta^*$ with probability $1-\alpha$.\\
We can note that the interval decreases as $n$ increases, and increases as $\alpha$ decreases.
\nb $\hat{\theta}_n$ is a random variable, while $\theta^*$ is not.\\

\example{Assymptotically Exact Confidence Interval when Flipping Coins}
\textit{This is an improvement on the bound produced in }\textbf{Example 5.3}.\\
Let $\X{\iid}\text{Bernoulli}(\theta^*)$ for some $\theta^*\in[0,1]$, $W\sim\text{Normal}(0,1)$ and define $\hat{\theta}_n:=\hat{\theta}_n(X_1,\dots,X_n)$.\\
We shall show that
$$\frac{\sqrt{n}(\hat{\theta}_n-\theta^*)}{\sqrt{\hat{\theta}_n(1-\hat{\theta}_n)}}\to_\mathcal{D}W$$
We know that $\var(X_1)=\theta^*(1-\theta^*)$.\\
By the \textit{Weak Law of Large Numbers} $\hat{\theta}_n\to_\prob\theta^*$ .\\
By the \textit{Central Limit Theorem}
$$\frac{\sqrt{n}(\hat{\theta}_n-\theta^*)}{\sqrt{\hat{\theta}_n(1-\hat{\theta}_n)}}\to_\mathcal{D}W$$
Define $Y_n=\dfrac{\sqrt{n}(\hat{\theta}_n-\theta^*)}{\sqrt{\theta^*(1-\theta^*)}}$ and $Z_n=\dfrac{\sqrt{\theta^*(1-\theta^*)}}{\sqrt{\hat{\theta}_n(1-\hat{\theta}_n)}}$.\\
By the \textit{Continuous Mapping Theorem} tells us that $Z_n\to_\mathcal{D}1$ and $Z_n\to_\prob1$.\\
Hence, by \textit{Slutsky's Theorem}
$$\frac{\sqrt{n}(\hat{\theta}_n-\theta^*)}{\sqrt{\hat{\theta}_n(1-\hat{\theta}_n)}}=Y_nZ_n\to_\mathcal{D}W$$
This gives us random interval
$$\left(\hat{\theta}_n-z_{\alpha/2}\sqrt{\frac{\hat{\theta}_n(1-\hat{\theta}_n)}{n}},\hat{\theta}_n+z_{\alpha/2}\sqrt{\frac{\hat{\theta}_n(1-\hat{\theta}_n)}{n}}\right)$$
This interval captures $\theta^*$ asymptotically (in $n$) with probability $1-\alpha$.\\
\nb $z_\alpha=\Phi^{-1}(1-\alpha)$ where $\Phi$ is the cumulative denisty function of a $\text{Normal}(0,1)$.

\section{The Fisher Information}

\remark{Motivation}
In the next part of the content we shall show that given $\X_n{\iid}f(\cdot;\theta^*)$ then for sufficiently regular models
\begin{enumerate}[label=\roman*)]
	\item There exists a lower bound on the achievable performance of any estimate of $\theta^*$.
	\item A scaled \& centered sequence of maximum likelihood estimators $\{\hat{\theta}_n(\X_n)\}$ become asymptotically normal as $n\to\infty$.
\end{enumerate}

\remark{Measuring Performance of Estimator}
We measure the performance of an estimator $\hat{\theta}$ in terms of variance, since its mean should be $\theta^*$. Lower variance indicates better performance.\\

\definition{The Score Function}
Let $\ell(\theta;x):=\ln f(x;\theta)$.\\
The \textit{Score Function} is a measure of the sensitivity of the likelihood function wrt $\theta$
$$\ell'(\theta;x):=\frac{d}{d\theta}\ell(\theta,;x)=\frac{\frac{d}{d\theta}f(x;\theta)}{f(x;\theta)}=\frac{L'(\theta;x)}{L(\theta;x)}$$

\remark{$\theta^*$ is a turning point of $\ell(\theta;x)$}
Note that under the \textit{Fisher Information Regularity Conitions} we have that $\forall\ \theta\in\Theta$
\[\begin{array}{rcl}
\expect(\ell'(\theta;X);\theta)&=&{\displaystyle\int_S\dfrac{\frac{d}{d\theta}f(x;\theta)}{f(x;\theta)}f(x;\theta)dx}\\
&=&{\displaystyle\int_S\frac{d}{d\theta}f(x;\theta)dx}\\
&=&{\displaystyle\frac{d}{d\theta}\int_Sf(x;\theta)dx}\\
&=&\frac{d}{d\theta}(1)\\
&=&0
\end{array}\]
This shows that we expect the derivative to equal $0$ at $\theta^*$. Further, this means $\theta^*$ is a turning point of the log-likelihood function (hopefully a maximum).\\

\example{Application of \textbf{Remark 6.3}}
Let $X\sim\text{Poisson}(\theta)$. Then $f_X(x;\theta)=\frac{\theta^x}{x!}e^{-\theta}\mathds{1}\{x\in\nats\}$.
\[\begin{array}{rrcl}
\implies&\ell(\theta;x)&=&-\theta+x\ln\theta-\ln x!\\
\implies&\ell'(\theta;x)&=&-1+\frac{x}{\theta}\\
\implies&\expect(\ell'(\theta;X);\theta)&=&-1+\frac{\theta}{\theta}\\
&&=&0
\end{array}\]

\definition{Fisher Information Regularity Conditions}
Let $\Theta$ be an open interval in $\reals$ and $f(x;\theta)$ be a pmf/pdf.\\
Below are conditions which a model is required to meet in order to be considered sufficiently regular such that \textit{Fisher Information} can be drawn from it.
\begin{enumerate}[label=\roman*)]
	\item Both $L'(\theta;x)=\frac{d}{d\theta}f(x;\theta)$ and $L''(\theta;x)=\frac{d^2}{d\theta^2}f(x;\theta)$ exist for any $x\in\mathcal{X}$.
	\item $\forall\ \theta\in\Theta$ the set $S:=\{x\in\mathcal{X}:\ f(x;\theta)>0\}$ does not depend on $\theta\in\Theta$.
	\item The idenity below exists
	$$\int_S\frac{d}{d\theta}f(x;\theta)dx=\frac{d}{d\theta}\int_Sf(x;\theta)dx=0$$
\end{enumerate}

\definition{Fisher Information}
\textit{Fisher Information} is a technique for measuring the amount of information that an observable random variable $X$ carries about an unknown parameter $\theta$ upon which the probability of $X$ depends.\\
Let $X\sim f(\cdots;\theta)$. Then the \textit{Fisher Information} for any $\theta\in\Theta$ is
$$I(\theta):=\expect(\ell'(\theta;X)^2;\theta)\geq0$$
\nb This is the \textit{Expectation of the score, squared} $\equiv$ \textit{Second moment of the score}.\\

\remarkk{Fisher Information}
\begin{enumerate}[label=\roman*)]
	\item \textit{Fisher Information} is a function of the parameter, $\theta$, not the data, $X$.
	\item $I(\theta)$ can be thought of as being the average \textit{information} brought by a single observation $X$ about $\theta$, assuming $X\sim f(\cdot;\theta)$.
	\item Since $\forall\ \theta\in\Theta,\ \expect(\ell'(\theta;X);\theta)=0$ then
	$$I(\theta)=\var(\ell'(\theta;X);\theta)$$
	The variance of the score.
\end{enumerate}

\example{Fisher Information of Poisson}
Let $X\sim\text{Poisson}(\theta)$.\\
From \textbf{Example 6.1} we kown that $\ell'(\theta;x)=-1+\frac{x}{\theta}$. Then
\[\begin{array}{rcl}
I(\theta)&=&\var(\ell'(\theta;X);\theta)\\
&=&\var\left(-1+\frac{X}{\theta};\theta\right)\\
&=&\var\left(\frac{X}{\theta};\theta\right)\\
&=&\frac{1}{\theta^2}\var(X;\theta)\\
&=&\frac{1}{\theta^2}.\theta\text{ since }X\sim\text{Poisson}(\theta)\\
&=&\frac{1}{\theta}
\end{array}\]

\theorem{Alternative Expression of Fisher Information}
Let $f(x;\theta)$ be a pmf/pdf which statisfies the conditions of \textbf{Definition 6.2}. If
$$\forall\ \theta\in\Theta\quad \int_\mathcal{X}\frac{d^2}{d\theta^2}f(x;\theta)dx=\frac{d}{d\theta}\int_\mathcal{X}\frac{d}{d\theta}f(x;\theta)dx$$
Then
$$I(\theta)=-\expect\left(\frac{d^2}{d\theta^2}\ell(\theta;X);\theta\right)$$
\nb ${\displaystyle\frac{d}{d\theta}\int_\mathcal{X}\frac{d}{d\theta}f(x;\theta)dx=0}$ by the regularity conditions.\\

\proof{Theorem 6.1}
By the \textit{Quotient Rule}
\[\begin{array}{rcl}
\frac{d^2}{d\theta^2}\ell(\theta;x)&=&\dfrac{d}{d\theta}\dfrac{\frac{d}{d\theta}f(x;\theta}{f(x;\theta)}\\
&=&\dfrac{\frac{d^2}{d\theta^2}f(x;\theta)}{f(x;\theta)}-\left(\dfrac{\frac{d}{d\theta}f(x;\theta)}{f(x;\theta)}\right)^2
\end{array}\]
Consequently
\[\begin{array}{rrcl}
&\expect\left(\frac{d^2}{d\theta^2}\ell(\theta;X);\theta\right)&=&{\displaystyle\int_S\frac{\frac{d^2}{d\theta^2}f(x;\theta)}{f(x;\theta}f(x;\theta)dx-\int_S\left(\frac{\frac{d}{d\theta}f(x;\theta)}{f(x;\theta)}\right)^2f(x;\theta)dx}\\
&&=&{\displaystyle\int_S\frac{d^2}{d\theta^2}f(x;\theta)dx-\int_S\left(\frac{d}{d\theta}\ell'(\theta;x)\right)^2f(x;\theta)dx}\\
&&=&0-\expect(\ell'(\theta;X)^2;\theta)\\
&&=&-I(\theta)\\
\implies&I(\theta)&=&-\expect\left(\frac{d^2}{d\theta^2}\ell(\theta;X);\theta\right)
\end{array}\]

\section{Efficiency and The Cramer-Rao Bound}

\definition{IID Score Function}
Let $\X{\iid}f(\cdot;\theta)$ for some $\theta\in\Theta$. Then the \textit{Score Function} is
$$\ell'_n(\theta;\x):=\frac{d}{d\theta}\ell_n(\theta;\x)\text{ where }l_n(\theta;\x):=\ln f_n(\x;\theta)=\sum_{i=1}^n\ell(\theta;x_i)$$

\definition{IID Fisher Information}
Let $\X{\iid}f(\cdot;\theta)$ for some $\theta\in\Theta$. Then the \textit{Fisher Information} is
$$I_n(\theta):=\expect(l'_n(\theta;\X)^2;\theta)=\var(l'_n(\theta;\X);\theta)$$

\theorem{Relationship between IID Fisher Information \& Fisher Information}
Consider the sitatution where $\forall\ \theta\in\Theta,\ f_n(\x;\theta)=\prod_{i=1}^nf(x_i;\theta)$. Then
$$\forall\ \theta\in\Theta,\ I_n(\theta)=nI(\theta)$$

\proof{Theorem 7.1}
Let $\X\overset{iid}{\sim}f(\cdot;\theta)$. Then
\[\begin{array}{rrcl}
&I_n(\theta)&=&\var(\ell'_n(\theta;\X);\theta)\\
&&=&\var\left(\sum\limits_{i=1}^n\ell'(\theta;X_i);\theta\right)\\
&&=&n\var\left(\sum\limits_{i=1}^n\ell'(\theta;X_1);\theta\right)\\
\implies&I_n(\theta)&=&nI(\theta)
\end{array}\]

\newpage
\setcounter{section}{-1}
\section{Appendix}

\definition{Gradient}
$$\nabla f(\pmb{\theta};\x):=\left(\dfrac{\partial f(\pmb{\theta};\x)}{\partial\theta_1},\dots,\dfrac{\partial f(\pmb{\theta};\x)}{\partial\theta_n}\right)$$

\definition{Hessian}
$$\nabla f(\pmb{\theta};\x):=\begin{pmatrix}
\dfrac{\partial^2 f(\pmb{\theta};\x)}{\partial\theta_1^2}&\dfrac{\partial^2 f(\pmb{\theta};\x)}{\partial\theta_1\partial\theta_2}&\dots&\dfrac{\partial^2 f(\pmb{\theta};\x)}{\partial\theta_n\theta_1}\\
\dfrac{\partial^2 f(\pmb{\theta};\x)}{\partial\theta_1^2}&\dfrac{\partial^2 f(\pmb{\theta};\x)}{\partial\theta_1\theta_2}&\dots&\dfrac{\partial^2 f(\pmb{\theta};\x)}{\partial\theta_n\theta_2}\\
\vdots&\vdots&\ddots&\vdots\\
\dfrac{\partial^2 f(\pmb{\theta};\x)}{\partial\theta_1\partial\theta_n}&\dfrac{\partial^2 f(\pmb{\theta};\x)}{\partial\theta_2\theta_n}&\dots&\dfrac{\partial^2 f(\pmb{\theta};\x)}{\partial\theta_n^2}
\end{pmatrix}$$

\subsection{Notation}

\begin{tabular}{|l|l|}
\hline
Notation&Denotes\\
\hline
$Z_n\to_{\prob}Z$&$\{Z_n\}_{n\in\nats}$ converges in \textit{Probabilitiy} to random variable $Z$.\\
$Z_n\to_\mathcal{D}Z$&$\{Z_n\}_{n\in\nats}$ converges in \textit{Distribution} to random variable $Z$.\\
$Z_n\to_{qm}Z$&$\{Z_n\}_{n\in\nats}$ converges in \textit{Quadratic Mean} to random variable $Z$.\\
$\theta\in\Theta\subseteq\reals^{d_\theta}$&Scalar or vector parameter characterising a probability distribution\\
$\hat{\theta}$&Estimation for the value of the parameter $\theta$\\
$\theta^*$&True value of the paramter $\theta$\\
$\prob$&Probability measure $\prob:\mathcal{F}\to[0,1]$\\
$\Omega$&Sample space\\
$X$&Scalar random variable\\
$\mathcal{F}$&Sigma field (Set of events)\\
$\chi$&Support of rv $XX$. A set set $X$ is definitely in it \ie $\prob(X\in\chi;\theta)=1$\\
$\X$&Vector consiting of scalar random variables\\
\hline
\end{tabular}

\subsection{R}
\begin{tabular}{|l|l|}
\hline
Command&Result\\
\hline
\textit{hist(a)}&Plots a histogram of the values in array $a$\\
\textit{mean(a)}&Returns the mean value of array $a$\\
\textit{rbinom(s,n,p)}&Samples $n$ of $Bi(n,p)$ random variables\\
\textit{rep(v,n)}&Produces an array of size $n$ where each entry has value $v$\\
$x\leftarrow v$&Maps value $v$ to variable $x$\\
\hline
\end{tabular}

\subsection{Probability Distributions}

\definition{Binomial Distribution}
Let $X$ be a discrete random variable modelled by a \textit{Binomial Distribution} with $n$ events and rate of success $p$.\\
\[\begin{array}{rcl}
p_X(k)&=&{n\choose k}p^k(1-p)^{n-k}\\
\expect(X)np=&\&&Var(X)=np(1-p)
\end{array}\]

\definition{Gamma Distribution}
Let $T$ be a continuous randmo variable modelled by a \textit{Gamma Distribution} with shape parameter $\alpha$ \& scale parameter $\lambda$. Then
\[\begin{array}{rcll}
f_T(x)&=&\dfrac{\lambda^\alpha x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)}&\mathrm{for\ }x>0\\
\expect(T)=\dfrac{\alpha}{\lambda}&\&&Var(T)=\dfrac{\alpha}{\lambda^2}
\end{array}\]
\nb $\alpha,\lambda>0$.\\

\definition{Exponential Distribution}
Let $T$ be a continuous random variable modelled by a \textit{Exponential Distribution} with parameter $\lambda$. Then
\[\begin{array}{rcl}
f_T(t)&=&\indicator\{t\geq0\}.\lambda e^{-\lambda t}\\
F_T(t)&=&\indicator\{t\geq0\}.\left(1-e^{-\lambda t}\right)\\
\expect(X)=\frac{1}{\lambda}&\&&Var(X)=\frac{1}{\lambda^2}
\end{array}\]
\nb Exponential Distribution is used to model the wait time between decays of a radioactive source.\\

\definition{Normal Distribution}
Let $X$ be a continuous random variable modelled by a \textit{Normal Distribution} with mean $\mu$ \& variance $\sigma^2$.\\
Then
\[\begin{array}{rcl}
f_X(x)&=&\dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\
F_X(x)&=&\dfrac{1}{\sqrt{2\pi\sigma^2}}\int\limits_{-\infty}^xe^{-\frac{(y-\mu)^2}{2\sigma^2}}dy\\
M_X(\theta)&=&e^{\mu\theta+\sigma^2\theta^2(1/2)}\\
\expect(X)=\mu&\&&Var(X)=\sigma^2
\end{array}\]

\definition{Poisson Distribution}
Let $X$ be a discrete random variable modelled by a \textit{Poisson Distribution} with parameter $\lambda$. Then
\[\begin{array}{rcll}
p_X(k)&=&\dfrac{e^{-\lambda}\lambda^k}{k!}&\mathrm{For\ }k\in\nats_0\\
\expect(X)=\lambda&\&&Var(X)=\lambda
\end{array}\]
\nb Poisson Distribution is used to model the number of radioactive decays in a time period.\\


\end{document}
